% Model test bed
% ned haughton
% <%from datetime import datetime as dt
print(dt.isoformat(dt.now(), sep=' '))%>

<<>>=
import xray
import numpy as np

from pals_utils.data import pals_xray_to_array

from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.preprocessing import PolynomialFeatures
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline

from ubermodel.models import get_pipeline_name
# from ubermodel.models import fit_pipe_multisite, simulate_pipe_multisite
from ubermodel.evaluate import evaluation_simulation
@

Cyclic train and test:

<<>>=
datapath = 'data/PALS/datasets'
datasets = ['Amplero', 'Blodgett', 'Bugac', 'ElSaler2', 'ElSaler',
            'Espirra', 'FortPeck', 'Harvard', 'Hesse', 'Howard',
            'Howlandm', 'Hyytiala', 'Kruger', 'Loobos', 'Merbleue',
            'Mopane', 'Palang', 'Sylvania', 'Tumba', 'UniMich']

met_paths = ['%s/met/%sFluxnet.1.4_met.nc' % (datapath, s) for s in datasets]
flux_paths = ['%s/flux/%sFluxnet.1.4_flux.nc' % (datapath, s) for s in datasets]

met_data = [xray.open_dataset(path) for path in met_paths]
flux_data = [xray.open_dataset(path) for path in flux_paths]
@

<<>>=
def test_pipeline_crossval(pipe, name, met_data, flux_data):
    assert isinstance(met_data, list), "Met data isn't a list"
    assert all([isinstance(ds, xray.Dataset) for ds in met_data]), \
        "At least one met dataset isn't an xray dataset"
    assert isinstance(flux_data, list), "Flux data isn't a list"
    assert all([isinstance(ds, xray.Dataset) for ds in flux_data]), \
        "At least one met dataset isn't an xray dataset"

    for i in range(len(met_data)):
        met_train = [m for j, m in enumerate(met_data) if j != i]
        flux_train = [m for j, m in enumerate(flux_data) if j != i]
        met_test = met_data[i]
        flux_test = flux_data[i]

        fit_pipe_multisite(pipe, met_train, flux_train)

        sim_data = simulate_pipe(pipe, met_test)

        eval_hash = evaluate_simulation(sim_data, flux_test, name)

        diagnostic_plots(sim_data, flux_data, name)


#@timeit
def fit_pipe_multisite(pipe, met_data, flux_data):

    met_array = np.concatenate([pals_xray_to_array(ds) for ds in met_data], 1)
    print([list(ds.data_vars) for ds in flux_data])
    flux_array = np.concatenate([pals_xray_to_array(ds) for ds in flux_data], 1)

    pipe.fit(X=met_array, y=flux_array)


#@timeit
def simulate_pipe(pipe, met_data):

    sim_data = pipe.fit(X=pals_xray_to_array(met_data))

    return sim_data


def diagnostic_plots(sim_data, flux_data, name):
    pass



@


## Instantaneous models

pseudo code:
```
    set up pipeline
    set pipeline name
    for each site in sites:
        fit pipeline on other sites
        simulate site
        evaluate site
        store eval results
        ? print diagnostic plots
    collate eval results and print summary plots (PLUMBER plots/multisim plots)
```

### Linear regression
- insensitive to scaling or PCA

<<>>=
from sklearn.linear_model import LinearRegression

pipe = make_pipeline(LinearRegression())
name = get_pipeline_name(pipe)

test_pipeline_crossval(pipe, name, met_data, flux_data)

sys.exit()

@

### Polynomial regression
- Only a slight improvement
    - because non-linearities are localised?

<<>>=
from sklearn.preprocessing import PolynomialFeatures

pipe = make_pipeline(PolynomialFeatures(2), LinearRegression())
name = get_pipeline_name(pipe, "poly2")
@

<<>>=
pipe = make_pipeline(PolynomialFeatures(5), LinearRegression())
name = get_pipeline_name(pipe, "poly5")
@

### SGD
- very sensitive to scaling. Not sensitive to PCA

<<>>=
from sklearn.linear_model import SGDRegressor

pipe = make_pipeline(StandardScaler(), SGDRegressor())
name = get_pipeline_name(pipe)
@

### Support Vector Machines
- Sensitive to scaling, not to PCA

<<>>=
from sklearn.svm import SVR

pipe = make_pipeline(SVR())
name = get_pipeline_name(pipe)
@

<<>>=
pipe = make_pipeline(StandardScaler(), SVR())
name = get_pipeline_name(pipe)
@

<<>>=
pipe = make_pipeline(StandardScaler(), PCA(), SVR())
name = get_pipeline_name(pipe)
@

<<>>=
pipe = make_pipeline(StandardScaler(), SVR(kernel="poly"))
name = get_pipeline_name(pipe, "polykernel")
@


### Multilayer Perceptron

<<>>=
# from sklearn.neural_network import MultilayerPerceptronRegressor
#
# pipe = make_pipeline(MultilayerPerceptronRegressor())
# name = get_pipeline_name(pipe)
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor())
# name = get_pipeline_name(pipe)
# @
#
# <<>>=
# pipe = make_pipeline(PCA(), MultilayerPerceptronRegressor())
# name = get_pipeline_name(pipe)
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), PCA(), MultilayerPerceptronRegressor())
# name = get_pipeline_name(pipe)
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(activation="logistic"))
# name = get_pipeline_name(pipe, "logisitic")
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(20,20,20,)))
# name = get_pipeline_name(pipe, "[20,20,20]")
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(10,10,)))
# name = get_pipeline_name(pipe, "[10,10]")
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(10,30,)))
# name = get_pipeline_name(pipe, "[10,30]")
# @
#
# <<>>=
# pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(20,20,)))
# name = get_pipeline_name(pipe, "[20,20]")
@


### K-nearest neighbours
- Not sensitive to scaling or PCA

<<>>=
from sklearn.neighbors import KNeighborsRegressor

pipe = make_pipeline(KNeighborsRegressor())
name = get_pipeline_name(pipe)
@

<<>>=
pipe = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors = 1000))
name = get_pipeline_name(pipe, "1000 neighbours")
@


### Decision Trees

<<>>=
from sklearn.tree import DecisionTreeRegressor

pipe = make_pipeline(DecisionTreeRegressor())
name = get_pipeline_name(pipe)
@

<<>>=
from sklearn.ensemble import ExtraTreesRegressor

pipe = make_pipeline(ExtraTreesRegressor())
name = get_pipeline_name(pipe)
@

<<>>=
pipe = make_pipeline(StandardScaler(), PCA(), ExtraTreesRegressor())
name = get_pipeline_name(pipe)
@


### Lagged linear regression

- need to make a proper wrapper for this.

<<>>=
#pipe = make_pipeline(LagFeatures(), LinearRegression())
#name = get_pipeline_name(pipe, "lag1")
@


### Markov lagging

- use one timestep's output as an input for the next timestep


PLUMBER plots/intercomparison
=============================


