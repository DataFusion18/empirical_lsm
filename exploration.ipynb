{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model exploration\n",
    "\n",
    "## Todo\n",
    "\n",
    "- add more metrics\n",
    "    - mutual info score\n",
    "- multi variate output\n",
    "- table of results\n",
    "- Rhys: Compare the functional form of empirical models to that of LSMs, see where they differ\n",
    "    - multivariate functional form\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "import xray\n",
    "import pandas as pd\n",
    "import tables\n",
    "import os, sys\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "from numbers import Number\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pals_utils as pu\n",
    "from pals_utils.stats import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "pl.rcParams['figure.figsize'] = (12.0, 3)\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import mpld3\n",
    "#mpld3.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Perceptron, SGDRegressor, LogisticRegression, PassiveAggressiveRegressor\n",
    "from sklearn.svm import SVR, NuSVR  #, LinearSVR\n",
    "# from sklearn.neural_network import MultilayerPerceptronRegressor # This is from a pull request: https://github.com/scikit-learn/scikit-learn/pull/3939\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "met_vars = ['SWdown', 'Tair', 'LWdown', 'Wind', 'Rainf', 'PSurf', 'Qair']\n",
    "met_data = xray.open_dataset('/home/naught101/phd/data/PALS/datasets/met/TumbaFluxnet.1.4_met.nc')\n",
    "met_df = met_data.to_dataframe().reset_index(['x','y','z']).ix[:, met_vars]\n",
    "\n",
    "flux_vars = ['Qh', 'Qle', 'Rnet', 'NEE']\n",
    "flux_data = xray.open_dataset('/home/naught101/phd/data/PALS/datasets/flux/TumbaFluxnet.1.4_flux.nc')\n",
    "flux_df = flux_data.to_dataframe().reset_index(['x','y']).ix[:, flux_vars]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('cache/'):\n",
    "    os.mkdir('cache')\n",
    "cache = pd.HDFStore('cache/cache.hdf5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def timeit(f):\n",
    "    def timed(*args, **kw):\n",
    "        ts = time.time()\n",
    "        result = f(*args, **kw)\n",
    "        te = time.time()\n",
    "        #print(f.__name__, 'took: {:2.4f} sec'.format(te-ts))        \n",
    "        return (result, te-ts)\n",
    "    return timed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@timeit\n",
    "def fit_pipeline(pipe, X, Y):\n",
    "    pipe.fit(X, Y)\n",
    "    \n",
    "    \n",
    "@timeit\n",
    "def get_pipeline_prediction(pipe, X):    \n",
    "    return(pipe.predict(X))\n",
    "\n",
    "\n",
    "def get_pipeline_name(pipe, suffix=None):\n",
    "    if suffix is not None:\n",
    "        return ', '.join(list(pipe.named_steps.keys()) + [suffix])\n",
    "    else:\n",
    "        return ', '.join(pipe.named_steps.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_tests_data(Y_pred, Y_validate, y_var):\n",
    "    # Sample plot\n",
    "    plot_data = pd.DataFrame({y_var+'_obs': Y_validate, y_var+'_pred': Y_pred})\n",
    " \n",
    "    # week 7 raw\n",
    "    pl.plot(plot_data[(70*48):(77*48)])\n",
    "    pl.legend(plot_data.columns)\n",
    "    pl.show()\n",
    "    \n",
    "    # fornightly rolling mean\n",
    "    pl.plot(pd.rolling_mean(plot_data, window=14*48))\n",
    "    pl.legend(plot_data.columns)\n",
    "    pl.show()\n",
    "    \n",
    "    #daily cycle\n",
    "    pl.plot(plot_data.groupby(np.mod(plot_data.index, 48)).mean())\n",
    "    pl.legend(plot_data.columns)\n",
    "    pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_metrics(Y_pred, Y_validate, metrics):\n",
    "    metric_data = OrderedDict()\n",
    "    for (n, m) in metrics.items():\n",
    "        metric_data[n] = m(Y_pred, Y_validate)\n",
    "    return metric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_pipeline(pipe, X=met_df, Y=flux_df, y_var=['Qh'], name=None, plot=False, cache=cache, clear_cache=False):\n",
    "    \"\"\"Top-level pipeline fitter and tester.\n",
    "    \n",
    "    Fits and predicts with a model, runs metrics, optionally runs some diagnostic plots.\n",
    "    \"\"\"\n",
    "    \n",
    "    if name is None:\n",
    "        name = get_pipeline_name(pipe)\n",
    "\n",
    "    if 'metric_data' in cache and not clear_cache:\n",
    "        if name in cache.metric_data.index:\n",
    "            print(\"Metrics already calculated for %s, skipping.\" % name)\n",
    "            return\n",
    "        metric_data = cache.metric_data\n",
    "    else:\n",
    "        metric_data = pd.DataFrame()\n",
    "    \n",
    "    Y = np.array(Y[y_var])\n",
    "    \n",
    "    train_len = (7*len(X)//10)\n",
    "    \n",
    "    # X_train, X_validate, Y_train, Y_validate = train_test_split(X, Y, train_size=0.7, random_state=0)\n",
    "    X_train = X[:train_len]\n",
    "    X_validate = X[train_len:]\n",
    "    Y_train = Y[:train_len]\n",
    "    Y_validate = Y[train_len:]\n",
    "    \n",
    "    \n",
    "    if 'predictions/' + name in cache and not clear_cache:\n",
    "        print('prediction already run for %s, skipping fit and predict' % name)\n",
    "        Y_pred = np.array(cache['predictions'][name])\n",
    "    else: \n",
    "        # Fit model\n",
    "        metric_data.ix[name, 't_fit'] = fit_pipeline(pipe, X_train, Y_train)[1]\n",
    "    \n",
    "        # Run model\n",
    "        (Y_pred, metric_data.ix[name, 't_pred']) = get_pipeline_prediction(pipe, X_validate)\n",
    "        # Some sklearn models return vector (n,) inputs as 2D arrays (n,1)\n",
    "        if len(Y_pred.shape) > 1:\n",
    "            Y_pred = Y_pred[:,0]\n",
    "        \n",
    "        cache.put('predictions/' + name, pd.DataFrame(Y_pred))\n",
    "    \n",
    "    for k, v in run_metrics(Y_pred, Y_validate, metrics).items():\n",
    "        metric_data.ix[name, k] = v\n",
    "    cache['metric_data'] = metric_data\n",
    "    cache.flush()\n",
    "    \n",
    "    # Plotting    \n",
    "    if plot:\n",
    "        [print('{:>10}:'.format(k), '{:.3f}'.format(v) if isinstance(v, Number) else v) for (k,v) in metric_data.items()]\n",
    "\n",
    "        plot_test_data(Y_pred, Y_validate, y_var)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fit - run - assess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_model_fit_path(hash):\n",
    "    return 'cache/model_fits/%s.pickle' % hash\n",
    "\n",
    "def fit_model_pipeline(pipe, land_data, name=None, cache=cache):\n",
    "    \"\"\"Top-level pipeline fitter.\n",
    "    \n",
    "    Fits a model, stores model and metadata.\n",
    "    \n",
    "    TODO: store domain metadata\n",
    "    \n",
    "    returns (pipe, model_hash)\n",
    "    \"\"\"\n",
    "    \n",
    "    if name is None:\n",
    "        name = get_pipeline_name(pipe)\n",
    "     \n",
    "    model_fit_id = joblib.hash((pipe, land_data))\n",
    "    \n",
    "    if ('model_fits/' + model_fit_id in cache) and not clear_cache:\n",
    "        print(\"Model %s already fitted for %s, loading from file.\" % name, land_data.name)\n",
    "        model_hash = cache['model_fits'][model_fit_id]['model_fit_hash']\n",
    "        with open(get_model_fit_path(model_hash), 'rb') as f:\n",
    "             pipe = pickle.load(f)\n",
    "    else:\n",
    "        if land_data.met is none or land_data.flux is none:\n",
    "            raise KeyError('missing met or flux data')\n",
    "        fit_time = fit_pipeline(pipe, land_data.met, land_data.flux)[1]\n",
    "        model_hash = joblib.hash(pipe)\n",
    "        cache['model_fits'][model_fit_id]['model_fit_hash'] = model_hash\n",
    "        cache['model_fits'][model_fit_id]['model_fit_time'] = fit_time\n",
    "        cache.flush()\n",
    "        with open(get_model_fit_path(model_hash), 'wb') as f:\n",
    "            pickle.dump(pipe, f)\n",
    "    \n",
    "    return pipe, model_hash\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sim_path(hash):\n",
    "    return 'cache/simulations/%s.pickle' % hash\n",
    "\n",
    "def simulate_model_pipeline(pipe, land_data, name=None, cache=cache):\n",
    "    \"\"\"Top-level pipeline predictor.\n",
    "    \n",
    "    runs model, caches model simulation.\n",
    "    \n",
    "    TODO: store domain metadata\n",
    "    \n",
    "    returns (sim_data, sim_hash)\n",
    "    \"\"\"\n",
    "    \n",
    "    if name is None:\n",
    "        name = get_pipeline_name(pipe)\n",
    "     \n",
    "    model_sim_id = joblib.hash((pipe, land_data))\n",
    "    \n",
    "    if ('simulations/' + model_fit_id in cache) and not clear_cache:\n",
    "        print(\"Model %s already simulated for %s, loading from file.\" % name, land_data.name)\n",
    "        sim_hash = cache['predictions'][model_sim_id]['sim_hash']\n",
    "        with open(get_sim_path(sim_hash), 'rb') as f:\n",
    "             sim_data = pickle.load(f)\n",
    "    else:\n",
    "        if land_data.met is none or land_data.flux is none:\n",
    "            raise KeyError('missing met or flux data')\n",
    "        sim_data = LandData(\"%s_%s\" % (get_pipeline_name(pipe), model_sim_id),\n",
    "                            land_data.domain_type, land_data.geo)\n",
    "        (sim_data.flux, fit_time) = get_pipeline_prediction(pipe, land_data.met, land_data.flux)\n",
    "        # TODO: If a simulation can produce more than one output for a given input, this won't be unique. Is that ok?\n",
    "        sim_hash = joblib.hash(sim_data)\n",
    "        cache['simulations'][model_fit_id]['sim_hash'] = sim_hash\n",
    "        cache['simulations'][model_fit_id]['model_predict_time'] = fit_time\n",
    "        cache.flush()\n",
    "        with open(get_model_fit_path(sim_hash), 'wb') as f:\n",
    "            pickle.dump(sim_data, f)\n",
    "            \n",
    "    return sim_data, sim_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Could make a wrapper around this so that you can just pass a hash, or a fit model, and auto-load the data.\n",
    "\n",
    "def evaluate_simulation(sim_data, land_data, y_vars, name, cache=cache):\n",
    "    \"\"\"Top-level simulation evaluator.\n",
    "    \n",
    "    Compares sim_data to land_data, using standard metrics. Stores the results in an easily accessible format.\n",
    "    \"\"\"\n",
    "        \n",
    "    eval_hash = joblib.hash((sim_data, land_data))\n",
    "\n",
    "    if 'metric_data' in cache and not clear_cache:\n",
    "        if eval_hash in cache.metric_data.index[0]:\n",
    "            print(\"Metrics already calculated for %s, skipping.\" % name)\n",
    "            return\n",
    "        metric_data = cache.metric_data\n",
    "    else:\n",
    "        metric_data = pd.DataFrame()\n",
    "    \n",
    "    for [y_var in y_vars]:\n",
    "        Y_sim = np.array(sim_data.flux[y_var])\n",
    "        Y_obs = np.array(land_data.flux[y_var])\n",
    "        \n",
    "        metric_data.ix[name, 'name'] = \"%s_%s\"  % (eval_hash, y_var)\n",
    "        metric_data.ix[name, 'sim_id'] = joblib.hash(sim_data)   \n",
    "        metric_data.ix[name, 'site'] = land_data.name   \n",
    "        metric_data.ix[name, 'var'] = y_var        \n",
    "\n",
    "        for k, v in run_metrics(Y_sim, Y_obs, metrics).items():\n",
    "            metric_data.ix[name, k] = v\n",
    "            \n",
    "    cache['metric_data'] = metric_data\n",
    "    cache.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Site setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LandData():\n",
    "    \"\"\"Land data storage mechanism.\n",
    "    \n",
    "    Stores met data, flux data, and domain data. \n",
    "    \n",
    "    TODO: This could be used to store model output too...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, domain_type, geo, met=None, flux=None, veg=None, soil=None):\n",
    "        self.name = name\n",
    "        self.domain_type = domain_type\n",
    "        self.geo = geo\n",
    "        self.met = met\n",
    "        self.flux = flux\n",
    "        self.veg = veg\n",
    "        self.soil = soil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "site_data = LandData('Tumbarumba', 'site', {\"lat\": 151, \"long\": -34})\n",
    "site_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression\n",
    "\n",
    "- insensitive to scaling or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(LinearRegression())\n",
    "test_pipeline(pipe, clear_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(StandardScaler(), LinearRegression())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(PCA(), LinearRegression())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(StandardScaler(), PCA(), LinearRegression())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial regression\n",
    "\n",
    "- Only a slight improvement\n",
    "    - because non-linearities are localised?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(PolynomialFeatures(2), LinearRegression())\n",
    "test_pipeline(pipe, name=get_pipeline_name(pipe, 'poly2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(PolynomialFeatures(5), LinearRegression())\n",
    "test_pipeline(pipe, name=get_pipeline_name(pipe, 'poly5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "met_df_with_lag = pd.concat([met_df, met_df.diff()], axis=1).dropna()\n",
    "met_df_with_lag.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.linalg.matrix_rank(np.array(met_df_with_lag[:40000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flux_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flux_df[1:40001].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(LinearRegression())\n",
    "test_pipeline(pipe, X=met_df_with_lag[:40000], Y=flux_df[1:40001], name=get_pipeline_name(pipe, 'lag1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "\n",
    "- very sensitive to scaling. Not sensitive to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(SGDRegressor())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), SGDRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(PCA(), SGDRegressor())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(StandardScaler(), PCA(), SGDRegressor())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_model(\"LogisticRegression\", LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test_model(\"PassiveAggressiveRegressor\", PassiveAggressiveRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- Sensitive to scaling, not to PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(SVR())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), SVR())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(StandardScaler(), PCA(), SVR())\n",
    "#test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), SVR(kernel='poly'))\n",
    "#\n",
    "test_pipeline(pipe, name=get_pipeline_name(pipe, 'polykernel'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(MultilayerPerceptronRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor())\n",
    "test_pipeline(pipe)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(PCA(), MultilayerPerceptronRegressor())\n",
    "test_pipeline(pipe)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), PCA(), MultilayerPerceptronRegressor())\n",
    "test_pipeline(pipe)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(activation='logistic'))\n",
    "test_pipeline(pipe, get_pipeline_name(pipe, 'logisitic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(20,20,20,)))\n",
    "test_pipeline(pipe, get_pipeline_name(pipe, \"[20,20,20]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(10,10,)))\n",
    "test_pipeline(pipe, get_pipeline_name(pipe, \"[10,10]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(10,30,)))\n",
    "test_pipeline(pipe, get_pipeline_name(pipe, \"[10,30]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), MultilayerPerceptronRegressor(hidden_layer_sizes=(20,20,)))\n",
    "test_pipeline(pipe, get_pipeline_name(pipe, \"[20,20]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-nearest neighbours \n",
    "\n",
    "- Not sensitive to scaling or PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(KNeighborsRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), KNeighborsRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(PCA(), KNeighborsRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), KNeighborsRegressor(n_neighbors=1000))\n",
    "test_pipeline(pipe, get_pipeline_name(pipe, \"1000 neighbours\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(DecisionTreeRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(ExtraTreesRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe = make_pipeline(StandardScaler(), PCA(), ExtraTreesRegressor())\n",
    "test_pipeline(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Metric results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache.metric_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normed_metrics = cache.metric_data - cache.metric_data.min()\n",
    "normed_metrics /= normed_metrics.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normed_metrics.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normed_metrics[['corr', 'nme', 'mbe', 'sd_diff']].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normed_metrics[['extreme_5','extreme_95']].plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?pd.DataFrame.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?base_repr("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.unsignedinteger(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash(KNeighborsRegressor(n_neighbors=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash(object())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joblib.hash(str(pipe.get_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l=pipe.named_steps['linearregression']\n",
    "l.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.array([321,12.3,1,1.4,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hash(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
